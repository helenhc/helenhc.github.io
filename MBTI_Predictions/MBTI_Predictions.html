<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_3ulb8fwmtdc6-7>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-7}.lst-kix_3ulb8fwmtdc6-7>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-7,lower-latin) ". "}ol.lst-kix_3ulb8fwmtdc6-2.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-2 0}.lst-kix_3ulb8fwmtdc6-6>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-6,decimal) ". "}ol.lst-kix_2fy51aqmnbtm-6.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-6 0}.lst-kix_3ulb8fwmtdc6-0>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-0,decimal) ". "}.lst-kix_3ulb8fwmtdc6-1>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-1}.lst-kix_3ulb8fwmtdc6-8>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-8,lower-roman) ". "}.lst-kix_adizeodejroe-0>li:before{content:"\0025cf  "}.lst-kix_adizeodejroe-1>li:before{content:"\0025cb  "}.lst-kix_3ulb8fwmtdc6-1>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-1,lower-latin) ". "}.lst-kix_3ulb8fwmtdc6-2>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-2}.lst-kix_2fy51aqmnbtm-3>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-3}.lst-kix_3ulb8fwmtdc6-2>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-2,lower-roman) ". "}.lst-kix_3ulb8fwmtdc6-8>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-8}ol.lst-kix_2fy51aqmnbtm-2.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-2 0}.lst-kix_3ulb8fwmtdc6-3>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-3,decimal) ". "}.lst-kix_3ulb8fwmtdc6-5>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-5,lower-roman) ". "}.lst-kix_3ulb8fwmtdc6-4>li:before{content:"" counter(lst-ctn-kix_3ulb8fwmtdc6-4,lower-latin) ". "}.lst-kix_2fy51aqmnbtm-3>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-3,decimal) ". "}ul.lst-kix_adizeodejroe-3{list-style-type:none}ul.lst-kix_adizeodejroe-2{list-style-type:none}.lst-kix_adizeodejroe-8>li:before{content:"\0025a0  "}.lst-kix_2fy51aqmnbtm-2>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-2,lower-roman) ". "}.lst-kix_2fy51aqmnbtm-4>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-4,lower-latin) ". "}ul.lst-kix_adizeodejroe-1{list-style-type:none}ul.lst-kix_adizeodejroe-0{list-style-type:none}.lst-kix_adizeodejroe-7>li:before{content:"\0025cb  "}.lst-kix_2fy51aqmnbtm-0>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-0,decimal) ". "}.lst-kix_2fy51aqmnbtm-1>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-1,lower-latin) ". "}.lst-kix_adizeodejroe-2>li:before{content:"\0025a0  "}ol.lst-kix_2fy51aqmnbtm-5.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-5 0}ol.lst-kix_3ulb8fwmtdc6-0.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-0 0}.lst-kix_adizeodejroe-3>li:before{content:"\0025cf  "}ol.lst-kix_3ulb8fwmtdc6-6.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-6 0}ol.lst-kix_2fy51aqmnbtm-8.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-8 0}.lst-kix_adizeodejroe-4>li:before{content:"\0025cb  "}.lst-kix_adizeodejroe-5>li:before{content:"\0025a0  "}.lst-kix_2fy51aqmnbtm-4>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-4}.lst-kix_2fy51aqmnbtm-1>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-1}.lst-kix_adizeodejroe-6>li:before{content:"\0025cf  "}ol.lst-kix_3ulb8fwmtdc6-3.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-3 0}.lst-kix_3ulb8fwmtdc6-4>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-4}.lst-kix_2fy51aqmnbtm-7>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-7}ol.lst-kix_3ulb8fwmtdc6-4.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-4 0}ol.lst-kix_3ulb8fwmtdc6-7{list-style-type:none}ol.lst-kix_3ulb8fwmtdc6-6{list-style-type:none}ol.lst-kix_3ulb8fwmtdc6-5{list-style-type:none}ol.lst-kix_3ulb8fwmtdc6-4{list-style-type:none}ol.lst-kix_2fy51aqmnbtm-1.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-1 0}ol.lst-kix_3ulb8fwmtdc6-3{list-style-type:none}ol.lst-kix_3ulb8fwmtdc6-2{list-style-type:none}ol.lst-kix_3ulb8fwmtdc6-1{list-style-type:none}ol.lst-kix_3ulb8fwmtdc6-0{list-style-type:none}.lst-kix_2fy51aqmnbtm-6>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-6}ol.lst-kix_2fy51aqmnbtm-4.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-4 0}ol.lst-kix_3ulb8fwmtdc6-7.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-7 0}.lst-kix_2fy51aqmnbtm-6>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-6,decimal) ". "}ul.lst-kix_adizeodejroe-8{list-style-type:none}.lst-kix_3ulb8fwmtdc6-5>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-5}.lst-kix_2fy51aqmnbtm-5>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-5,lower-roman) ". "}ol.lst-kix_2fy51aqmnbtm-7.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-7 0}ul.lst-kix_adizeodejroe-7{list-style-type:none}ul.lst-kix_adizeodejroe-6{list-style-type:none}ul.lst-kix_adizeodejroe-5{list-style-type:none}ul.lst-kix_adizeodejroe-4{list-style-type:none}ol.lst-kix_2fy51aqmnbtm-0.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-0 0}ol.lst-kix_3ulb8fwmtdc6-8.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-8 0}ol.lst-kix_2fy51aqmnbtm-7{list-style-type:none}ol.lst-kix_2fy51aqmnbtm-6{list-style-type:none}ol.lst-kix_2fy51aqmnbtm-8{list-style-type:none}ol.lst-kix_3ulb8fwmtdc6-1.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-1 0}.lst-kix_2fy51aqmnbtm-2>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-2}ol.lst-kix_2fy51aqmnbtm-3{list-style-type:none}.lst-kix_2fy51aqmnbtm-7>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-7,lower-latin) ". "}ol.lst-kix_2fy51aqmnbtm-2{list-style-type:none}.lst-kix_3ulb8fwmtdc6-6>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-6}ol.lst-kix_2fy51aqmnbtm-5{list-style-type:none}.lst-kix_2fy51aqmnbtm-8>li:before{content:"" counter(lst-ctn-kix_2fy51aqmnbtm-8,lower-roman) ". "}ol.lst-kix_2fy51aqmnbtm-4{list-style-type:none}.lst-kix_2fy51aqmnbtm-8>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-8}.lst-kix_2fy51aqmnbtm-0>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-0}ol.lst-kix_2fy51aqmnbtm-1{list-style-type:none}.lst-kix_2fy51aqmnbtm-5>li{counter-increment:lst-ctn-kix_2fy51aqmnbtm-5}ol.lst-kix_3ulb8fwmtdc6-5.start{counter-reset:lst-ctn-kix_3ulb8fwmtdc6-5 0}ol.lst-kix_2fy51aqmnbtm-0{list-style-type:none}ol.lst-kix_2fy51aqmnbtm-3.start{counter-reset:lst-ctn-kix_2fy51aqmnbtm-3 0}.lst-kix_3ulb8fwmtdc6-3>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-3}ol.lst-kix_3ulb8fwmtdc6-8{list-style-type:none}.lst-kix_3ulb8fwmtdc6-0>li{counter-increment:lst-ctn-kix_3ulb8fwmtdc6-0}ol{margin:0;padding:0}table td,table th{padding:0}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c2{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left;height:11pt}.c7{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c8{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c21{padding-top:16pt;padding-bottom:4pt;line-height:2.0;page-break-after:avoid;text-align:left}.c20{padding-top:0pt;padding-bottom:0pt;line-height:2.0;text-align:center}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c5{font-size:10pt;font-family:"Times New Roman";font-style:italic;font-weight:400}.c14{color:#434343;text-decoration:none;vertical-align:baseline;font-style:normal}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c6{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c11{font-weight:400;font-size:10pt;font-family:"Arial"}.c29{font-weight:700;font-size:14pt;font-family:"Arial"}.c16{font-weight:400;font-size:11pt;font-family:"Arial"}.c3{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c22{font-weight:400;font-size:14pt;font-family:"Times New Roman"}.c4{font-size:12pt;font-family:"Times New Roman";font-weight:700}.c19{font-weight:400;font-size:10pt;font-family:"Times New Roman"}.c12{orphans:2;widows:2;height:11pt}.c15{orphans:2;widows:2}.c25{font-weight:400;font-family:"Times New Roman"}.c18{font-weight:700;font-family:"Times New Roman"}.c30{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c13{border:1px solid black;margin:5px}.c26{width:33%;height:1px}.c27{font-size:11pt}.c10{margin-left:36pt}.c28{height:11pt}.c24{background-color:#ffffff}.c23{font-size:10pt}.c9{text-indent:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c24 c30"><p class="c2"><span class="c0"></span></p><p class="c15 c20"><span class="c4">Classification of Myer-Briggs Personality Type</span></p><h3 class="c21 c15" id="h.i2onzvqlg8bb"><span class="c18">Introduction</span></h3><p class="c7"><span class="c3">Our project revolves around predicting Myers-Briggs Personality type. The dataset that was used was one pulled from Reddit. This dataset consists of three different columns, consisting of the personality type of the user, the body of the text within their post, and the subreddit that this post was found in. The personality type was any of the 16 personalities, e.g. INTJ, INTP, INFP, etc. There are four main types of personalities, and within each umbrella, there are four specific personalities. In other words, Analysts, Diplomats, Sentinels, and Explorers are the four kinds of personalities that there are. Within each kind of personality, it is further broken down into four specific personalities, where for example, analysts are further reduced down to INTJ, INTP, ENTJ, or ENTP. These four umbrella terms each give way to four other personality types, which is why there are sixteen in total.</span><sup class="c3"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c3">&nbsp;</span></p><h3 class="c15 c21" id="h.nolns03ag3p4"><span class="c18">Data</span></h3><p class="c7"><span class="c3">There are approximately 1,555,110 rows within the dataset. </span><span class="c3">There were only 23 rows where the text column was considered missing in the dataset, but b</span><span class="c3">ecause the dataset is so large, we are able to drop it without imputing</span><span class="c3">.</span><span class="c3">&nbsp;After dropping those 23 rows, we sampled 500,000 rows </span><span class="c0">for all of our models. We used 400,000 for the training set, 50,000 for the validation set, and 50,000 for the testing set. &nbsp;</span></p><h3 class="c21 c15" id="h.wf5q4chujtmu"><span class="c18">Exploratory Data Analysis</span></h3><p class="c7"><span class="c0">To further understand the data that we are working with, we performed some preliminary exploratory data analysis. We first analyzed the distribution of the 16 personality types in our dataset. </span></p><p class="c7 c10"><span class="c6 c4">Distribution of 16 Personality Types</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 426.50px; height: 272.79px;"><img alt="" src="images/image4.jpg" style="width: 426.50px; height: 272.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c3">We noticed that in our dataset, we have more introverted type samples than we do of extraverted types. To combat this imbalance, we analyzed the data distribution again, but this time using the broader personality types: Analysts, Diplomats, Sentinels, and Explorers. </span></p><p class="c2"><span class="c6 c4"></span></p><p class="c2"><span class="c4 c6"></span></p><p class="c2"><span class="c6 c4"></span></p><p class="c2"><span class="c6 c4"></span></p><p class="c2"><span class="c6 c4"></span></p><p class="c7"><span class="c6 c4">Distribution of type group in dataset</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 409.65px; height: 263.50px;"><img alt="" src="images/image3.jpg" style="width: 409.65px; height: 263.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c10"><span class="c6 c4">Distribution of type group in population</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 419.50px; height: 263.74px;"><img alt="" src="images/image7.jpg" style="width: 419.50px; height: 263.74px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c3">From looking at these two plots, we noticed that Analysts and Diplomats were heavily more represented in our dataset than Sentinels or Explorers. However, compared to the population distribution of personality types found on </span><span class="c3">Career Planning</span><sup class="c3"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c0">, the data bias towards Diplomats and Analysts that we are seeing here is not typical. In the actual population distribution, Sentinels and Explorers are actually a lot more common compared to Diplomats and Analysts, which is the opposite of what we found in our dataset. Due to this observation, we sampled 500k rows with a more even distribution of types when building our model to help prevent prediction bias.</span></p><p class="c7 c10"><span class="c6 c4">Histogram of character count</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 376.50px; height: 230.03px;"><img alt="" src="images/image8.jpg" style="width: 376.50px; height: 230.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c0">After understanding the distribution of our dataset better, we further explored the data performing univariate and bivariate aggregations. We also wanted to look at the distribution of character count in posts and observed that 99.5% of our data has a character count of less than 2500. To further break this down, we also plotted EDA on this. </span></p><p class="c7"><span class="c6 c4">Average character count of post by type group</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 345.43px; height: 222.27px;"><img alt="" src="images/image1.jpg" style="width: 345.43px; height: 222.27px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0"><br>We wanted to take this observation further by seeing if personality type has a correlation with the length of a Reddit post. It appears that Explorers on average tend to use slightly more characters than the other type groups in their posts. Diplomats tend to use the least amount of characters. However, all 4 groups seem to fall around the 200-250 characters range.</span></p><p class="c15 c17"><span class="c6 c4"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Average punctuation usage in post by type group</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.50px; height: 264.21px;"><img alt="" src="images/image6.jpg" style="width: 391.50px; height: 264.21px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c0">We also wanted to know if certain types utilize more punctuation on average than others. We looked at their average punctuation usage and compared it across all 4 groups. Similar to the previous analysis, we noticed that Explorers tend to use more punctuation and Diplomats use the least. </span></p><p class="c7"><span class="c6 c4">Average emoji usage in post by type group</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 390.50px; height: 258.08px;"><img alt="" src="images/image2.jpg" style="width: 390.50px; height: 258.08px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c0">We also plotted to see if emoji usage is correlated with certain types. We were surprised to find that unlike the previous two observations, emoji usage showed the complete opposite. Diplomats use emojis at almost double the average frequency than Explorers do. Our findings indicate that there may be a correlation between emoji usage and punctuation usage between the groups.</span></p><p class="c8"><span class="c0">With our analysis developed through our observations in exploring the data, we decided to focus on these features when building our initial baseline model.</span></p><h3 class="c21 c15" id="h.fc1ldsyjvof4"><span class="c18">Models</span></h3><p class="c7"><span class="c0">For this project, we wanted to develop a model with the predictive task of classification. More specifically we want to develop a model to predict personality type. Initially, we wanted to predict each of the 16 types for each test user, however, we decided to focus on predicting type groups to reduce complexity. To evaluate our model, we will be using an accuracy score as this will let us know what proportion of the data is our model is predicting the correct answers.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c8"><span class="c4">Baseline Model</span></p><p class="c8 c10"><span class="c0">In order to develop a good model, we need to start off with a basic baseline model. </span></p><p class="c8"><span class="c0">For this step, we decided to use Logistic Regression with no specified regularization coefficient to classify the 4 different categories since it is easy to implement and fairly fast. The features we used for this model are the features we analyzed in our exploratory data analysis. For each post, we extract these features: the character count, emoji count, and punctuation count. As we mentioned from the previous section, we noticed that there may be some correlation between these features with the different personality type groups which prompted us to build a model on these features. </span></p><p class="c7"><span class="c0">To extract these features for our model, we needed to do some preprocessing. To obtain the character count features, we applied a function to the text column and calculated the length of the post. To obtain the emoji count feature, we looked up the most common 25 emojis used in text posts and applied a function to count the number of times emojis appeared in a post. To obtain the punctuation count feature, we applied a function to count the number of occurrences of any punctuation in the English dictionary for each post.</span></p><p class="c7"><span class="c0">To assess the validity of our model, we split our data into train, validation, and test sets with a sample of 500k rows. We tried to make our data sample as balanced as possible with the four personality groups by having similar sample counts. We then trained our Logistic Regression model with these 3 features and with a personality type group as our target output using the training set. We then validated our outputs by running our model on our validation set. We got an accuracy score of only 25.3% which is only slightly better than random with random being 25%.</span></p><p class="c7"><span class="c0">As we expected, due to this being the baseline model, it did not perform very well. In terms of the different classifiers we will be discussing in the next section, we decided to utilize the same preprocessing methods for all of them to ensure fairness when comparing our evaluation metrics. We only preprocessed the text within the body column by normalizing, with the use of making the text all lowercase, getting rid of punctuation and stop words, and stemming. After this, we were able to vectorize this by using a tf-idf vectorizer. In the next few sections, we will go into depth on the kinds of classifiers we experimented with and the steps we took to decide on the final model.</span></p><p class="c2 c9"><span class="c0"></span></p><p class="c8"><span class="c6 c4">KNN (K-Nearest Neighbors)</span></p><p class="c7"><span class="c3">To try and improve our model from the first iteration, we tried a KNN model. We chose this because it </span><span class="c3">is very simple and easy to use and it only has one hyperparameter. The problem we encountered is that it is really slow especially when you have large input values. </span><span class="c3">To try to solve this problem, we use Principal Component Analysis (PCA) to identify the combinations of columns that account most variance in the data. </span><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup><span class="c0">This certainly improved the runtime speed, but the accuracy for my test result is around 50%. We believed the KNN did not do so well because the data itself is not balanced. </span></p><p class="c2"><span class="c0"></span></p><p class="c8"><span class="c6 c4">SVM</span></p><p class="c7"><span class="c3">The next model we developed was an SVM model. Initially, when we had tried our model w</span><span class="c3">ith sixteen possible predictions for personality type, we were getting a poor accuracy metric at around 0.28. We thought this might be due to having too many classifications, so we decided to narrow this down by just looking at the four major personality types (analyst, diplomat, sentinel, and explorer). Changing the predicted classifications from sixteen to four greatly improved the classifiers we were looking at. For example, changing just the classification improved our SVM model&rsquo;s accuracy from a 0.28 to 0.57, which is still a huge improvement from both the 0.28 in the initial SVM model as well as the baseline model we have developed. This is fairly decent, since a completely random SVM model would have an accuracy of 0.25 since there are only four classifications to choose </span><span class="c3">from. </span><span class="c0">SVM is based on finding the ideal hyperplane that would best separate the features to develop the best and accurate predictions. I thought this classifier would work well because people within the same personality type are supposed to be similar in terms of personality, so I had believed that there would be a pretty clear margin of separation to work with. But one disadvantage to this is that there are a lot less occurrences of the Sentinel and Explorer type personalities, which might make this particular model overfit to the data points that we do have a lot of, which would be the Diplomats and Analysts. Further, one other drawback to this classifier is that it does not work well with larger datasets. But intuitively, it would make sense that the more posts we have for a user, the better prediction we might make for their personality type. So, even with a dataset with large amounts of information per user, SVM does not utilize the advantage that comes with a larger dataset very well. </span></p><p class="c2"><span class="c0"></span></p><p class="c8"><span class="c6 c4">Neural Networks</span></p><p class="c7"><span class="c3">There is a saying that the more data you feed into the neural network, the better result it gets with time. In this case, we were fortunate to stumble upon such a large dataset. We wanted to try if it was possible to get a much better result compared to traditional machine learning models.</span><sup><a href="#cmnt2" id="cmnt_ref2">[b]</a></sup><span class="c3">&nbsp;</span><span class="c3">But sometimes it is really difficult to figure out why neural networks perform either very well or very poorly. For example, when comparing with a decision tree algorithm, you can actually see the decision boundary or choices it makes. This provided some explanation to the model and you can adjust it from there.</span><sup><a href="#cmnt3" id="cmnt_ref3">[c]</a></sup><span class="c0">&nbsp;Another downside to the neural network is that it requires a lot of data. In order to perform well it requires a lot of computational power. </span></p><p class="c8"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We decided to use two hidden layers. This is because two hidden layers is enough to approximate any function that contains continuous mapping. We do not know how many neurons we need. It heavily depends on the data set. If we have too few neurons in the hidden layer, then it will result in underfitting. If we have too many neurons then it leads to overfitting. Another downside for having too many neurons is that it will take too long to train the network. We tested different neuron values in order to obtain the one with the highest performance. </span></p><p class="c7"><span class="c0">We define our network with two fully connected ReLu activated layers with 64 hidden neurons. This decision is through trial and errors. The third and last layer will be my output layer. It will consist of size 4. &nbsp;This layer will use a softmax activation function and will output a four dimensional vector. Each dimension will be the probability of certain input belonging to that class. The optimizer we use is Stochastic Gradient Descent. The loss function we use is categorical cross entropy. We use accuracy for my metric to monitor our network.</span></p><p class="c7"><span class="c0">Softmax activation is similar to the sigmoid function such that it squash the output value between 0 and 1. It also divided each output by the sum of all output. This gives us a discrete probability for each individual class.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 233.09px; height: 117.89px;"><img alt="" src="images/image9.png" style="width: 233.09px; height: 117.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span class="c3">The sum of the output probabilities will be 1. &nbsp;This is really helpful for us to interpret the output for the classification task. </span><sup><a href="#cmnt4" id="cmnt_ref4">[d]</a></sup><span class="c0">&nbsp;</span></p><h3 class="c21 c15" id="h.yp70ql8ktqpo"><span class="c14 c22">Results</span></h3><p class="c8 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 338.92px; height: 239.24px;"><img alt="" src="images/image5.png" style="width: 338.92px; height: 239.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c3 c24">At the end of the training and validating we decided to choose the neural network model as our best model. We have attained a training accuracy of 67% and test accuracy of 61.5% using this model. We observe that maximum validation accuracy is achieved at around 20 epochs. After that, we observe that there is a decrease in validation accuracy and an increase in training accuracy. This implies that the model is getting better at classifying the training data, but making consistently worse predictions when it encounters new, previously unseen data. If this were a balanced dataset, a random attribution of labels would, using simple probability, result in 25% accuracy. But since this dataset is not balanced, the accuracy of a random classifier might be on the lesser side. Compared to the baseline logistic regression model we had built earlier that had only achieved 25.3% accuracy, the neural network model performs much better.</span></p><h3 class="c21 c15" id="h.b7gi3khnjvuc"><span class="c25">Related Works</span></h3><p class="c7"><span class="c3">We gathered this dataset from Zenodo, which had been gathered using Google Big Query.</span><sup class="c3"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c3">&nbsp;We had originally gotten this idea to do a project on the Myers-Briggs Personality dataset because we had come across a dataset about this topic on Kaggle</span><sup class="c3"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c0">. One drawback from using the Kaggle dataset was that there were only around 8,600 rows in this dataset, which was not large enough. This dataset that we found on Zenodo solved this issue, since there are over 1.5 million rows to work with. </span></p><p class="c7"><span class="c3">There is extensive research on the field of MBTI prediction. </span><span class="c3">There is one study that was published at Stanford, which tried to predict the Myers-Briggs Type Indicator based off of text, similar to what we are attempting to do here</span><span class="c3">.</span><sup class="c3"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c0">&nbsp;They used the Kaggle dataset that I had mentioned earlier, which drew from postings on an online forum. They utilized a recurrent neural network (RNN) for this classification, training on the four binary classifiers (analysts, diplomats, sentinels, and explorers). After training, they took the average of the class probability predictions and rounded, giving them either 0 or 1, which gave them the classification. One task they did that I found fascinating was that they had attempted to generalize to other social networks, by giving the assumption that a classifier would improve its metric if given more social media posts for a single user. So to further delve into this assumption, they also scraped 30,000 tweets from Donald Trump&rsquo;s Twitter account and tried to predict what personality type he was, and had deduced that the final prediction was ESTP, which is verified to be his actual personality type according to experts in this field. </span></p><p class="c7"><span class="c3">Another piece of literature on predicting MBTI was found from another study done at Stanford, where they had tried to predict personality type not on the kinds of words that were posted, but rather the sentence structure and flow.</span><sup class="c3"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c0">&nbsp;They utilized this approach with the assumption that introverts might display a different tone in their writing compared to that of their extrovert counterparts. They manually built the dataset, by using books from ten different famous authors for each personality type. In total, they had around 250,000 rows in their dataset after preprocessing this data. Testing out a variety of different models, like an unsupervised SVD, RNN with LSTM, and a feed-forward neural network, they had found that the RNN with LSTM had the best performance, with an accuracy of up to 37%. </span></p><p class="c7"><span class="c3">Further, there is also work based on predicting personality types based off of Reddit data, much like the work that we had done. This was done from the University of Zagreb, where they also gathered their data using the database off of Google Big Query, and from there, selected all users who have any mentions of MBTI type within their flair field.</span><sup class="c3"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span class="c0">&nbsp;Much like the other work I had already mentioned, they had framed this prediction task as four independent binary classifications, testing it on a support vector machine (SVM), regularized-logistic regression, and a three-layer multilayer perceptron (MLP). The baseline model that they were comparing this to was the majority class classifier. The MLP model performed the best, giving them an accurate personality prediction about 42% of the time, which is significantly higher than their baseline prediction accuracy of 25%. </span></p><p class="c2"><span class="c0"></span></p><hr class="c26"><div><p class="c1 c15"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c19">&nbsp;&ldquo;Personality Types.&rdquo; </span><span class="c5">16 Personalities</span><span class="c6 c19">, www.16personalities.com/personality-types. </span></p><p class="c1 c12"><span class="c6 c11"></span></p></div><div><p class="c1 c15"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c19">&nbsp;</span><span class="c5">How Rare Is Your Personality Type?</span><span class="c6 c19">&nbsp;www.careerplanner.com/MB2/TypeInPopulation.cfm. </span></p><p class="c1 c12"><span class="c6 c11"></span></p></div><div><p class="c1 c15"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c19">&nbsp;Storey, Dylan. &ldquo;Myers Briggs Personality Tags on Reddit Data.&rdquo; </span><span class="c5">Zenodo</span><span class="c6 c19">, 30 July 2018, zenodo.org/record/1482951?fbclid=IwAR12-eZaiqRE_UNrtKp-8aepdKX31gh_ONJMiC0kQER7AZlKZ7Rojuj7TSM. </span></p><p class="c1 c12"><span class="c6 c11"></span></p></div><div><p class="c1 c15"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c19">&nbsp;J, Mitchell. &ldquo;(MBTI) Myers-Briggs Personality Type Dataset.&rdquo; </span><span class="c5">Kaggle</span><span class="c6 c19">, 22 Sept. 2017, www.kaggle.com/datasnaek/mbti-type?fbclid=IwAR3PnLr5KR7LXufVIy0aUnm-O2ysyBxTvfd8iY3kOao4zQNMd8ehw44_kPs. </span></p><p class="c1 c12"><span class="c6 c11"></span></p></div><div><p class="c1 c15"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c6 c19">&nbsp;Hernandez, Rayne, and Ian Scott Knight. &ldquo;Predicting Myers-Briggs Type Indicator with Text Classification.&rdquo; https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6839354.pdf. </span></p><p class="c1 c12"><span class="c6 c11"></span></p></div><div><p class="c1 c15"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c23">&nbsp;</span><span class="c19">Liu, Gus, and Anthony Ma. &ldquo;Neural Networks in Predicting Myers Brigg Personality Type From Writing Style.&rdquo; https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2736946.pdf. </span></p></div><div><p class="c1 c15"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c23">&nbsp; </span><span class="c19">Gjurkovic, Matej, and Jan &Scaron;najder. &ldquo;Reddit: A Gold Mine for Personality Prediction.&rdquo; https://www.aclweb.org/anthology/W18-1112.pdf. </span></p></div><div class="c13"><p class="c1"><a href="#cmnt_ref1" id="cmnt1">[a]</a><span class="c6 c16">PCA</span></p></div><div class="c13"><p class="c1"><a href="#cmnt_ref2" id="cmnt2">[b]</a><span class="c6 c16">Explain and justify your decision to use the model you proposed</span></p></div><div class="c13"><p class="c1"><a href="#cmnt_ref3" id="cmnt3">[c]</a><span class="c6 c16">What are the strengths</span></p><p class="c1"><span class="c6 c16">and weaknesses of the different models being compared?</span></p><p class="c1"><span class="c6 c16">pro:</span></p><p class="c1"><span class="c6 c16">1) Much better result compared to traditional machine learning model</span></p><p class="c1 c28"><span class="c6 c16"></span></p><p class="c1"><span class="c6 c16">cons:</span></p><p class="c1"><span class="c6 c16">1) Difficult to figure out explanation.</span></p><p class="c1"><span class="c6 c16">2) Required a lot of data</span></p></div><div class="c13"><p class="c1"><a href="#cmnt_ref4" id="cmnt4">[d]</a><span class="c6 c16">Reason why I choose this activation function.</span></p></div></body></html>